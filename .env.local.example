# Readable - Environment Variables Example
# Copy this file to .env.local and fill in your API keys

# ============================================
# LLM Provider API Keys (REQUIRED - at least one)
# ============================================
# At minimum, you need ONE of these API keys based on your chosen provider
# Set LLM_PROVIDER=openai|anthropic|gemini to choose the default provider

OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GEMINI_API_KEY=...

# Default LLM Provider (openai, anthropic, or gemini)
LLM_PROVIDER=openai

# ============================================
# Weaviate Configuration (REQUIRED)
# ============================================
WEAVIATE_URL=https://your-cluster.weaviate.cloud
WEAVIATE_API_KEY=your-api-key

# ============================================
# Optional: Kontext.dev (Persona Personalization)
# ============================================
# KONTEXT_API_KEY=your-kontext-api-key
# KONTEXT_API_URL=https://api.kontext.dev
# KONTEXT_SYSTEM_PROMPT_PATH=/v1/context/get

# ============================================
# Optional: PostHog Analytics
# ============================================
# POSTHOG_KEY=your-posthog-key

# ============================================
# Optional: Semantic Scholar API
# ============================================
# SEMANTIC_SCHOLAR_KEY=your-semantic-scholar-key

# ============================================
# Optional: arXiv Configuration
# ============================================
# ARXIV_CONTACT_EMAIL=your-email@example.com
# ARXIV_API_BASE_URL=https://export.arxiv.org/api/query
# AR5IV_BASE_URL=https://ar5iv.org/html

# ============================================
# Optional: Ingestion Services
# ============================================
# GROBID_URL=http://localhost:8070
# DEEPSEEK_OCR_URL=https://your-ocr-endpoint
# RUNPOD_API_KEY=your-runpod-key
# RUNPOD_ENDPOINT_ID=your-endpoint-id
# ENABLE_OCR_FALLBACK=true
# INGEST_PDF_TEXT_THRESHOLD=1000

# ============================================
# Optional: Custom Certificates
# ============================================
# NODE_EXTRA_CA_CERTS=certs/kontext-ca.crt

# ============================================
# Optional: OpenAI Organization/Project
# ============================================
# OPENAI_ORGANIZATION=org-...
# OPENAI_PROJECT=proj-...

# ============================================
# Optional: Model Overrides
# ============================================
# Models are automatically selected based on task type (see src/server/llm-config/models.json)
# Only override if you need different models than the defaults:
# OPENAI_QA_MODEL=gpt-4o
# ANTHROPIC_SUMMARY_MODEL=claude-3-5-sonnet-20241022
# GEMINI_QA_MODEL=gemini-1.5-pro

# ============================================
# Notes - Configuration Defaults
# ============================================
# Most configuration has sensible defaults defined in src/server/config/defaults.ts
# Only set these if you need to override the defaults:
#
# Timeouts (milliseconds): {SERVICE}_TIMEOUT_MS
#   Examples: OPENAI_TIMEOUT_MS=90000, KONTEXT_TIMEOUT_MS=10000
#   See defaults.ts for all available timeout keys and default values
#
# API Base URLs: {SERVICE}_API_BASE_URL or {SERVICE}_URL
#   Examples: OPENAI_API_BASE_URL=https://proxy.example.com/v1
#   Only needed for custom endpoints (proxy servers, self-hosted)
#   See defaults.ts for all default URLs
